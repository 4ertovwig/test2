многопоточная сортировка большого файла(файл не влезает в оперативу, например)... размер занимаемой оперативки и количество потоков задается из командной строки

в общем, почему сделано так а не иначе... 

входной файл читается более мелкими частями(настолько мелкими, которыми позволяет заданная максимально возможная используемая память ), которые грузим в память, сортируем, записываем во временные файлы... затем эти временные файлы сливаем в один... на это уходит львиная часть времени т.к. само слияние файлов не самое быстрое...

в итоге смог родить более-менее вменяемую параллельную сортировку, выигрывает у std::sort по скорости на 2-4 потоках пример в 1,7 раз на моем компе

про ключи читать файл run.sh ( используемое число для памяти указывается в мегабайтах, например, -m 352 значит, что максимальная возможная занимаемая память равна 352 мб )... ну и если задатьпамять больше чем размер файла, то не знаю что будет, не проверял (в условии задания файл всегда большой). 

у меня используется gcc 5.4.0, но вроде на gcc 4.8 должно все собраться, проверял с помощью rextester.com

boost не использовался т.к. я так понял, что нельзя чтобы он использовался по заданию

собирать - make
удалять временные файлы и ненужные бинарники - make clean
